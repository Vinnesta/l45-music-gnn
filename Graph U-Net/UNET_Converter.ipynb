{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyNI5I1L142Mw6E8cJUEScqn"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import time as TimeLib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, degree, remove_self_loops, to_dense_adj, dense_to_sparse\n",
    "from torch.nn import Linear, ReLU, BatchNorm1d, Module, Sequential\n",
    "from torch_scatter import scatter\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Batch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "import fluidsynth\n",
    "import mido\n",
    "import pretty_midi\n",
    "import networkx as nx\n",
    "from mido import MidiFile, MidiTrack, Message\n",
    "NORMAL_GRAPH = 89\n",
    "MINI_GRAPH = 21\n",
    "SEQ_FEATURE_LENGTH = 20"
   ],
   "metadata": {
    "id": "qyzr3bRjoHR-"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs:  1\n",
      "GPU  0  name:  NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# Print the number of available GPUs\n",
    "print(\"Number of available GPUs: \", num_gpus)\n",
    "\n",
    "# Print the name of each GPU\n",
    "for i in range(num_gpus):\n",
    "    print(\"GPU \", i, \" name: \", torch.cuda.get_device_name(i))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track 0: \n",
      "0\n",
      "Track 1: \n",
      "15788\n",
      "[(51, 60, 1049), (51, 0, 93), (35, 44, 86), (51, 54, 9), (39, 55, 168), (35, 0, 32), (39, 0, 129), (42, 52, 2), (42, 0, 115), (52, 76, 32), (51, 0, 7), (47, 56, 9), (52, 0, 24), (37, 61, 149), (54, 68, 0), (47, 0, 16), (52, 77, 36), (54, 0, 58), (54, 51, 0), (52, 0, 8), (52, 60, 41), (47, 57, 22), (54, 0, 10), (47, 0, 94), (46, 58, 46), (46, 0, 174), (52, 0, 10), (37, 0, 3), (51, 68, 0), (44, 35, 34)]\n",
      "[(51, 1.6886551024520846, 10.53108590593801), (51, 0.0, 0.9336425064368302), (35, 1.2383470751315289, 0.863368339285671), (51, 1.5197895922068763, 0.09035250062291905), (39, 1.547933843914411, 1.6865800116278225), (35, 0.0, 0.32125333554815666), (39, 0.0, 1.2950525089285065), (42, 1.4635010887918067, 0.02007833347175979), (42, 0.0, 1.154504174626188), (52, 2.1389631297726406, 0.32125333554815666), (51, 0.0, 0.07027416715115926), (47, 1.5760780956219458, 0.09035250062291905), (52, 0.0, 0.24094000166111748), (37, 1.7167993541596194, 1.4958358436461043), (54, 1.9138091161123627, 0.0), (47, 0.0, 0.16062666777407833), (52, 2.167107381480175, 0.3614100024916762), (54, 0.0, 0.5822716706810339), (54, 1.435356837084272, 0.0), (52, 0.0, 0.08031333388703916), (52, 1.6886551024520846, 0.4116058361710757), (47, 1.6042223473294805, 0.22086166818935768), (54, 0.0, 0.10039166735879895), (47, 0.0, 0.9436816731727101), (46, 1.632366599037015, 0.46180166985047516), (46, 0.0, 1.7468150120431016), (52, 0.0, 0.10039166735879895), (37, 0.0, 0.030117500207639685), (51, 1.9138091161123627, 0.0), (44, 0.9850488097637161, 0.3413316690199164)]\n"
     ]
    }
   ],
   "source": [
    "# Load the MIDI file\n",
    "file = mido.MidiFile('example.midi')\n",
    "msg_cnt = 0\n",
    "data_tuples = []\n",
    "for i, track in enumerate(file.tracks):\n",
    "    print('Track {}: {}'.format(i, track.name))\n",
    "    other_time = 0\n",
    "    for msg in track:\n",
    "        other_time += msg.time\n",
    "        if msg.type == 'note_on':\n",
    "            note_tuple = (msg.note - 20, msg.velocity, other_time)\n",
    "            other_time = 0\n",
    "            data_tuples.append(note_tuple)\n",
    "            msg_cnt += 1\n",
    "    print(msg_cnt)\n",
    "\n",
    "velocity_values = [t[1] for t in data_tuples]\n",
    "timestep_values = [t[2] for t in data_tuples]\n",
    "\n",
    "velocity_mean = 0 #np.mean(velocity_values)\n",
    "velocity_std = np.std(velocity_values)\n",
    "timestep_mean = 0 #np.mean(timestep_values)\n",
    "timestep_std = np.std(timestep_values)\n",
    "print(data_tuples[:30])\n",
    "initial_data_tuples = data_tuples.copy()\n",
    "data_tuples = [(t[0], (t[1] - 0) / velocity_std, (t[2] - 0) / timestep_std) for t in data_tuples]\n",
    "print(data_tuples[:30])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_piano_graph(num_nodes):\n",
    "    edges = [(i, i + 1) for i in range(1, num_nodes - 1)] + [(i + 1, i) for i in range(1, num_nodes - 1)]\n",
    "    edges += [(0, i) for i in range(1, num_nodes)] + [(i, 0) for i in range(1, num_nodes)]\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long, device='cuda:0').t().contiguous()\n",
    "\n",
    "    edge_features = [1] * (len(edges) - 2 * (num_nodes - 1)) + [0] * (2 * (num_nodes - 1))\n",
    "    edge_attr = torch.tensor(edge_features, dtype=torch.float, device='cuda:0').view(-1, 1)\n",
    "    print(edge_attr.shape)\n",
    "    return edge_index, edge_attr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([350, 1])\n",
      "torch.Size([78, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "edge_index, edge_attr = create_piano_graph(NORMAL_GRAPH)\n",
    "edge_index_mini, edge_attr_mini = create_piano_graph(MINI_GRAPH)\n",
    "x = torch.zeros((len(data_tuples), NORMAL_GRAPH, SEQ_FEATURE_LENGTH), dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "for i, (note, velocity, timestep) in enumerate(data_tuples):\n",
    "    if i > 0:\n",
    "        x[i, :, :-1] = x[i-1, :, 1:]  # Shift all rows one to the front\n",
    "        x[i, :, -1] = x[i-1, :, -1]\n",
    "    # Normalize velocity and timestep\n",
    "    x[i, note, -1] = velocity\n",
    "    x[i, 0, -1] = timestep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "graph_data_list = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    data = Data(x=x[i], edge_index=edge_index, edge_attr=edge_attr,  y=x[i].view(1, 89, 20))\n",
    "    graph_data_list.append(data)\n",
    "train_dataset = graph_data_list[:1000]\n",
    "val_dataset = graph_data_list[1000:1250]\n",
    "test_dataset = graph_data_list[1250:1500]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class MPNNLayer(MessagePassing):\n",
    "    def __init__(self, emb_dim, edge_dim, aggr='add'):\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        self.mlp_msg = Sequential(\n",
    "            Linear(2*emb_dim + edge_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
    "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
    "          )\n",
    "\n",
    "        self.mlp_upd = Sequential(\n",
    "            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
    "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
    "          )\n",
    "\n",
    "    def forward(self, h, edge_index, edge_attr):\n",
    "        print(\"FORWARD FUNCITON\")\n",
    "        print(\"F1\", edge_attr.shape)\n",
    "        print(\"F2\",edge_index.shape)\n",
    "        print(\"F3\",h.shape)\n",
    "        out = self.propagate(edge_index, h=h, edge_attr=edge_attr)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j, edge_attr):\n",
    "        print(h_i.shape)\n",
    "        print(h_j.shape)\n",
    "        print(edge_attr.shape)\n",
    "        msg = torch.cat([h_i, h_j, edge_attr], dim=-1)\n",
    "        return self.mlp_msg(msg)\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "\n",
    "    def update(self, aggr_out, h):\n",
    "        upd_out = torch.cat([h, aggr_out], dim=-1)\n",
    "        return self.mlp_upd(upd_out)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class MPNNModelEnDecoder(Module):\n",
    "    def __init__(self, num_layers, emb_dim, in_feature_dim, edge_dim, out_node_count, out_feature_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_feature_dim = out_feature_dim\n",
    "        self.out_node_count = out_node_count\n",
    "        # Linear projection for initial node features\n",
    "        # dim: d_n -> d\n",
    "        self.lin_in = Linear(in_feature_dim, emb_dim)\n",
    "\n",
    "        # Stack of MPNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(MPNNLayer(emb_dim, edge_dim, aggr='add'))\n",
    "\n",
    "        # Global pooling/readout function `R` (mean pooling)\n",
    "        # PyG handles the underlying logic via `global_mean_pool()`\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        # Linear prediction head\n",
    "        # dim: d -> num_nodes * out_dim\n",
    "        self.lin_pred = Linear(emb_dim, out_node_count * out_feature_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        h = self.lin_in(data.x)  # (n, d_n) -> (n, d)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            h = h + conv(h, data.edge_index, data.edge_attr)  # (n, d) -> (n, d)\n",
    "            # Note that we add a residual connection after each MPNN layer\n",
    "\n",
    "        print(\"we are here in pool:\")\n",
    "        print(h.shape)\n",
    "        print(data.batch.shape)\n",
    "        h_graph = self.pool(h, data.batch)  # (n, d) -> (batch_size, d)\n",
    "\n",
    "        out_feature_matrix = self.lin_pred(h_graph)  # (batch_size, d) -> (batch_size, num_nodes * out_dim)\n",
    "\n",
    "        # Reshape the output to the desired matrix shape (batch_size, num_nodes, out_dim)\n",
    "        out_matrix = out_feature_matrix.view(-1, self.out_node_count, self.out_feature_dim)\n",
    "        out_matrix[:, 0, :] = self.relu(out_matrix[:, 0, :])\n",
    "\n",
    "        out_matrix[:, 1:, :] = out_matrix[:, 1:, :].clamp(0, 1)\n",
    "        return out_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# class Decoder(Module):\n",
    "#     def __init__(self, in_feature_dim=20, out_dim=20, in_nodes=10, out_nodes=89):\n",
    "#         super().__init__()\n",
    "#\n",
    "#         self.in_feature_dim = in_feature_dim\n",
    "#         self.out_dim = out_dim\n",
    "#         self.in_nodes = in_nodes\n",
    "#         self.out_nodes = out_nodes\n",
    "#\n",
    "#         self.fc1 = Linear(in_feature_dim * in_nodes, 256)\n",
    "#         self.fc2 = Linear(256, 512)\n",
    "#         self.fc3 = Linear(512, out_dim * out_nodes)\n",
    "#         self.relu = ReLU()\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, self.in_nodes * self.in_feature_dim)  # Flatten input matrix\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#\n",
    "#         # Reshape the output to the desired matrix shape (batch_size, out_nodes, out_dim)\n",
    "#         out_matrix = x.view(-1, self.out_nodes, self.out_dim)\n",
    "#\n",
    "#         return out_matrix\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class GraphUNet(Module):\n",
    "    def __init__(self, num_layers=4, emb_dim=64, in_feature_dim=SEQ_FEATURE_LENGTH, edge_dim=1, out_node_count = MINI_GRAPH, out_feature_dim=SEQ_FEATURE_LENGTH, in_node_count=NORMAL_GRAPH):\n",
    "        super().__init__()\n",
    "        self.encoder = MPNNModelEnDecoder(num_layers, emb_dim, in_feature_dim , edge_dim, out_node_count, out_feature_dim)\n",
    "        self.decoder = MPNNModelEnDecoder(num_layers, emb_dim, out_feature_dim, edge_dim, in_node_count , in_feature_dim)\n",
    "        # self.model = nn.Sequential(self.encoder, self.decoder)\n",
    "\n",
    "    # def forward(self, data):\n",
    "    #     # out = self.model(data)\n",
    "    #     print(\"inja model UNET hast ke baadesh dare encoder ro call mikoen\")\n",
    "    #     print(data)\n",
    "    #     encoded_matrix = self.encoder(data)\n",
    "    #     encoded_data_list = []\n",
    "    #     for x in encoded_matrix:\n",
    "    #         data = Data(x=x, edge_index=edge_index_mini, edge_attr=edge_attr_mini)\n",
    "    #         encoded_data_list.append(data)\n",
    "    #     encoded_batch = Batch.from_data_list(encoded_data_list)\n",
    "    #     decoded_matrix = self.decoder(encoded_batch)\n",
    "    #\n",
    "    #     return decoded_matrix\n",
    "\n",
    "    def forward(self, data):\n",
    "        # out = self.model(data)\n",
    "        print(\"FIRST DATA IS:\")\n",
    "        print(data.x.shape)\n",
    "        h_encoded = self.encoder(data)\n",
    "        print(\"HHHHHHHHHHHHHHHH\")\n",
    "        print(h_encoded.shape)\n",
    "        data_decoded = data.clone()\n",
    "        data_decoded.x = h_encoded.view(-1, self.encoder.out_feature_dim)\n",
    "        print(data_decoded.x.shape)\n",
    "        data_decoded.edge_index = edge_index_mini\n",
    "        data_decoded.edge_attr = edge_attr_mini\n",
    "\n",
    "        h_decoded = self.decoder(data_decoded)\n",
    "\n",
    "        return h_decoded\n",
    "\n",
    "    # def forward(self, data):\n",
    "    #     # Encoding\n",
    "    #     h_encoded = self.encoder(data)\n",
    "    #\n",
    "    #     # Decoding\n",
    "    #     data_decoded = data.clone()\n",
    "    #     data_decoded.x = h_encoded.view(-1, self.encoder.out_feature_dim)\n",
    "    #\n",
    "    #     h_decoded = self.decoder(data_decoded)\n",
    "    #\n",
    "    #     return h_decoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 3. Initialize the model, optimizer, and loss function\n",
    "model = GraphUNet().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data)\n",
    "        loss = F.mse_loss(y_pred, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_loader.dataset)\n",
    "\n",
    "def eval(model, loader, device):\n",
    "    model.eval()\n",
    "    error = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(data)\n",
    "            # Mean Absolute Error using std (computed when preparing data)\n",
    "            error += (y_pred * std - data.y * std).abs().sum().item()\n",
    "    return error / len(loader.dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def run_experiment(model, model_name, train_loader, val_loader, test_loader, n_epochs=100):\n",
    "\n",
    "    print(f\"Running experiment for {model_name}, training on {len(train_loader.dataset)} samples for {n_epochs} epochs.\")\n",
    "\n",
    "    print(\"\\nModel architecture:\")\n",
    "    print(model)\n",
    "    total_param = 0\n",
    "    for param in model.parameters():\n",
    "        total_param += np.prod(list(param.data.size()))\n",
    "    print(f'Total parameters: {total_param}')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Adam optimizer with LR 1e-3\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # LR scheduler which decays LR when validation metric doesn't improve\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.9, patience=5, min_lr=0.00001)\n",
    "\n",
    "    print(\"\\nStart training:\")\n",
    "    best_val_error = None\n",
    "    perf_per_epoch = [] # Track Test/Val MAE vs. epoch (for plotting)\n",
    "    t = TimeLib.time()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # Call LR scheduler at start of each epoch\n",
    "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Train model for one epoch, return avg. training loss\n",
    "        loss = train(model, train_loader, optimizer, device)\n",
    "\n",
    "        # Evaluate model on validation set\n",
    "        val_error = eval(model, val_loader, device)\n",
    "\n",
    "        if best_val_error is None or val_error <= best_val_error:\n",
    "            # Evaluate model on test set if validation metric improves\n",
    "            test_error = eval(model, test_loader, device)\n",
    "            best_val_error = val_error\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            # Print and track stats every 10 epochs\n",
    "            print(f'Epoch: {epoch:03d}, LR: {lr:5f}, Loss: {loss:.7f}, '\n",
    "                  f'Val MAE: {val_error:.7f}, Test MAE: {test_error:.7f}')\n",
    "\n",
    "        scheduler.step(val_error)\n",
    "        perf_per_epoch.append((test_error, val_error, epoch, model_name))\n",
    "\n",
    "    t = TimeLib.time() - t\n",
    "    train_time = t/60\n",
    "    print(f\"\\nDone! Training took {train_time:.2f} mins. Best validation MAE: {best_val_error:.7f}, corresponding test MAE: {test_error:.7f}.\")\n",
    "\n",
    "    return best_val_error, test_error, train_time, perf_per_epoch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for GraphUNet, training on 1000 samples for 100 epochs.\n",
      "\n",
      "Model architecture:\n",
      "GraphUNet(\n",
      "  (encoder): MPNNModelEnDecoder(\n",
      "    (lin_in): Linear(in_features=20, out_features=64, bias=True)\n",
      "    (convs): ModuleList(\n",
      "      (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
      "    )\n",
      "    (lin_pred): Linear(in_features=64, out_features=420, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (decoder): MPNNModelEnDecoder(\n",
      "    (lin_in): Linear(in_features=20, out_features=64, bias=True)\n",
      "    (convs): ModuleList(\n",
      "      (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
      "    )\n",
      "    (lin_pred): Linear(in_features=64, out_features=1780, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      ")\n",
      "Total parameters: 348952\n",
      "\n",
      "Start training:\n",
      "FIRST DATA IS:\n",
      "torch.Size([89, 20])\n",
      "FORWARD FUNCITON\n",
      "F1 torch.Size([350, 1])\n",
      "F2 torch.Size([2, 350])\n",
      "F3 torch.Size([89, 64])\n",
      "torch.Size([350, 64])\n",
      "torch.Size([350, 64])\n",
      "torch.Size([350, 1])\n",
      "FORWARD FUNCITON\n",
      "F1 torch.Size([350, 1])\n",
      "F2 torch.Size([2, 350])\n",
      "F3 torch.Size([89, 64])\n",
      "torch.Size([350, 64])\n",
      "torch.Size([350, 64])\n",
      "torch.Size([350, 1])\n",
      "FORWARD FUNCITON\n",
      "F1 torch.Size([350, 1])\n",
      "F2 torch.Size([2, 350])\n",
      "F3 torch.Size([89, 64])\n",
      "torch.Size([350, 64])\n",
      "torch.Size([350, 64])\n",
      "torch.Size([350, 1])\n",
      "FORWARD FUNCITON\n",
      "F1 torch.Size([350, 1])\n",
      "F2 torch.Size([2, 350])\n",
      "F3 torch.Size([89, 64])\n",
      "torch.Size([350, 64])\n",
      "torch.Size([350, 64])\n",
      "torch.Size([350, 1])\n",
      "we are here in pool:\n",
      "torch.Size([89, 64])\n",
      "torch.Size([89])\n",
      "HHHHHHHHHHHHHHHH\n",
      "torch.Size([1, 21, 20])\n",
      "wtfff torch.Size([21, 20])\n",
      "FORWARD FUNCITON\n",
      "F1 torch.Size([78, 1])\n",
      "F2 torch.Size([2, 78])\n",
      "F3 torch.Size([21, 64])\n",
      "torch.Size([78, 64])\n",
      "torch.Size([78, 64])\n",
      "torch.Size([78, 1])\n",
      "FORWARD FUNCITON\n",
      "F1 torch.Size([78, 1])\n",
      "F2 torch.Size([2, 78])\n",
      "F3 torch.Size([21, 64])\n",
      "torch.Size([78, 64])\n",
      "torch.Size([78, 64])\n",
      "torch.Size([78, 1])\n",
      "FORWARD FUNCITON\n",
      "F1 torch.Size([78, 1])\n",
      "F2 torch.Size([2, 78])\n",
      "F3 torch.Size([21, 64])\n",
      "torch.Size([78, 64])\n",
      "torch.Size([78, 64])\n",
      "torch.Size([78, 1])\n",
      "FORWARD FUNCITON\n",
      "F1 torch.Size([78, 1])\n",
      "F2 torch.Size([2, 78])\n",
      "F3 torch.Size([21, 64])\n",
      "torch.Size([78, 64])\n",
      "torch.Size([78, 64])\n",
      "torch.Size([78, 1])\n",
      "we are here in pool:\n",
      "torch.Size([21, 64])\n",
      "torch.Size([89])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected index [89] to be smaller than self [1] apart from dimension 0 and to be smaller size than src [21]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m GraphUNet(num_layers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, emb_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, in_feature_dim \u001B[38;5;241m=\u001B[39m SEQ_FEATURE_LENGTH, edge_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, out_node_count \u001B[38;5;241m=\u001B[39m MINI_GRAPH, out_feature_dim\u001B[38;5;241m=\u001B[39mSEQ_FEATURE_LENGTH, in_node_count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m89\u001B[39m)\n\u001B[0;32m      2\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(model)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[1;32m----> 3\u001B[0m best_val_error, test_error, train_time, perf_per_epoch \u001B[38;5;241m=\u001B[39m \u001B[43mrun_experiment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\n\u001B[0;32m     10\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m RESULTS[model_name] \u001B[38;5;241m=\u001B[39m (best_val_error, test_error, train_time)\n\u001B[0;32m     12\u001B[0m df_temp \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(perf_per_epoch, columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest MAE\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVal MAE\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "Cell \u001B[1;32mIn[20], line 29\u001B[0m, in \u001B[0;36mrun_experiment\u001B[1;34m(model, model_name, train_loader, val_loader, test_loader, n_epochs)\u001B[0m\n\u001B[0;32m     26\u001B[0m lr \u001B[38;5;241m=\u001B[39m scheduler\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mparam_groups[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Train model for one epoch, return avg. training loss\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# Evaluate model on validation set\u001B[39;00m\n\u001B[0;32m     32\u001B[0m val_error \u001B[38;5;241m=\u001B[39m \u001B[38;5;28meval\u001B[39m(model, val_loader, device)\n",
      "Cell \u001B[1;32mIn[19], line 12\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_loader, optimizer, device)\u001B[0m\n\u001B[0;32m     10\u001B[0m data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 12\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmse_loss(y_pred, data\u001B[38;5;241m.\u001B[39my)\n\u001B[0;32m     14\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[18], line 35\u001B[0m, in \u001B[0;36mGraphUNet.forward\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m     32\u001B[0m data_decoded\u001B[38;5;241m.\u001B[39medge_index \u001B[38;5;241m=\u001B[39m edge_index_mini\n\u001B[0;32m     33\u001B[0m data_decoded\u001B[38;5;241m.\u001B[39medge_attr \u001B[38;5;241m=\u001B[39m edge_attr_mini\n\u001B[1;32m---> 35\u001B[0m h_decoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_decoded\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m h_decoded\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[16], line 35\u001B[0m, in \u001B[0;36mMPNNModelEnDecoder.forward\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(h\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28mprint\u001B[39m(data\u001B[38;5;241m.\u001B[39mbatch\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 35\u001B[0m h_graph \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpool\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (n, d) -> (batch_size, d)\u001B[39;00m\n\u001B[0;32m     37\u001B[0m out_feature_matrix \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlin_pred(h_graph)  \u001B[38;5;66;03m# (batch_size, d) -> (batch_size, num_nodes * out_dim)\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# Reshape the output to the desired matrix shape (batch_size, num_nodes, out_dim)\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\torch_geometric\\nn\\pool\\glob.py:63\u001B[0m, in \u001B[0;36mglobal_mean_pool\u001B[1;34m(x, batch, size)\u001B[0m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39mdim, keepdim\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     62\u001B[0m size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(batch\u001B[38;5;241m.\u001B[39mmax()\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m size\n\u001B[1;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mscatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmean\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\torch_geometric\\utils\\scatter.py:78\u001B[0m, in \u001B[0;36mscatter\u001B[1;34m(src, index, dim, dim_size, reduce)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reduce \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     77\u001B[0m     count \u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39mnew_zeros(dim_size)\n\u001B[1;32m---> 78\u001B[0m     \u001B[43mcount\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscatter_add_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnew_ones\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m     count \u001B[38;5;241m=\u001B[39m count\u001B[38;5;241m.\u001B[39mclamp(\u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     81\u001B[0m     index \u001B[38;5;241m=\u001B[39m broadcast(index, src, dim)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected index [89] to be smaller than self [1] apart from dimension 0 and to be smaller size than src [21]"
     ]
    }
   ],
   "source": [
    "model = GraphUNet(num_layers=4, emb_dim=64, in_feature_dim = SEQ_FEATURE_LENGTH, edge_dim=1, out_node_count = MINI_GRAPH, out_feature_dim=SEQ_FEATURE_LENGTH, in_node_count=89)\n",
    "model_name = type(model).__name__\n",
    "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
    "    model,\n",
    "    model_name,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    n_epochs=100\n",
    ")\n",
    "RESULTS[model_name] = (best_val_error, test_error, train_time)\n",
    "df_temp = pd.DataFrame(perf_per_epoch, columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])\n",
    "DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_output_list = []\n",
    "\n",
    "# Process each data element and store the output\n",
    "for data in graph_data_list:\n",
    "    data = data.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "    graph_output_list.append(output.cpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_tuples(x):\n",
    "    extracted_data_tuples = []\n",
    "    for i in range(0, len(x)):\n",
    "        if i > 0:\n",
    "            prev_x = x[i - 1].squeeze()\n",
    "        else:\n",
    "            prev_x = torch.zeros_like(x[i]).squeeze()\n",
    "        curr_x = x[i].squeeze()\n",
    "        diff_mask = torch.abs(prev_x[:, -1] - curr_x[:, -1])\n",
    "        mask = torch.zeros_like(diff_mask)\n",
    "        mask[1:] = 1  # exclude 0th index\n",
    "        masked_diff = diff_mask * mask  # apply the mask\n",
    "        changed_note = torch.argmax(masked_diff).item()\n",
    "        # Get the velocity and timestep\n",
    "        velocity = curr_x[changed_note, -1].item()\n",
    "        timestep = curr_x[0, -1].item()\n",
    "\n",
    "        extracted_data_tuples.append((changed_note, velocity, timestep))\n",
    "\n",
    "    return extracted_data_tuples\n",
    "all_data_tuples = []\n",
    "all_data_tuples = extract_tuples(graph_output_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data_tuples[0:5])\n",
    "print(all_data_tuples[0:5])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tuples_to_midi(data_tuples, filename, velocity_std, timestep_std):\n",
    "    # Initialize a new MIDI file and track\n",
    "    midi_file = MidiFile()\n",
    "    track = MidiTrack()\n",
    "    midi_file.tracks.append(track)\n",
    "\n",
    "    # Process tuples and add MIDI messages to the track\n",
    "    COUNTER = 0\n",
    "    for note, velocity, timestep in data_tuples:\n",
    "        # De-normalize velocity and timestep\n",
    "        velocity = round(velocity * velocity_std)\n",
    "        timestep = round(timestep * timestep_std)\n",
    "\n",
    "        ini_tuple = initial_data_tuples[COUNTER]\n",
    "\n",
    "        if note != ini_tuple[0] or velocity != ini_tuple[1] or timestep != ini_tuple[2]:\n",
    "            print(initial_data_tuples[COUNTER])\n",
    "            print(note, velocity, timestep)\n",
    "            print()\n",
    "        COUNTER+=1\n",
    "        # Adjust note range\n",
    "        note = note + 20\n",
    "\n",
    "        # Clamp values to the valid MIDI range\n",
    "        note = max(21, min(108, note))\n",
    "        velocity = max(0, min(127, velocity))\n",
    "        #velocity = 64\n",
    "\n",
    "        # Add MIDI messages to the track\n",
    "        track.append(Message('note_on', note=note, velocity=velocity, time=timestep))\n",
    "\n",
    "    # Save the MIDI file\n",
    "    midi_file.save(filename)\n",
    "tuples_to_midi(data_tuples, 'output.mid', velocity_std, timestep_std)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
