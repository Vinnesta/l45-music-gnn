{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-lZnwwf7NyU",
        "outputId": "942be48d-3571-41e5-e171-83116dd8b994"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mange\\miniconda3\\envs\\music-gnn\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import mido\n",
        "from utils.midi_tools02 import mid2arry, convert2binary, arry2mid\n",
        "from utils.data_processing import get_tonnetz_edge_index, slice_temporal_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmaG2s-J9BbZ"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0-D7kWW7wIw"
      },
      "source": [
        "## MIDI processing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mmrG_zz1729S"
      },
      "outputs": [],
      "source": [
        "def get_midi_timesteps(filename):\n",
        "    midi = mido.MidiFile(filename)\n",
        "    timesteps = 0\n",
        "    for track in midi.tracks:\n",
        "        for msg in track:\n",
        "            timesteps += msg.time\n",
        "    return timesteps\n",
        "\n",
        "def midi_to_tensor(filename, binary_velocity=False):\n",
        "    mid = mido.MidiFile(filename)\n",
        "    # Extract information about the notes being played\n",
        "    max_timesteps = get_midi_timesteps(filename)\n",
        "    tensor = np.zeros((max_timesteps, 128))\n",
        "    previous_note = [0] * 128\n",
        "    timesteps = 0\n",
        "    for track in mid.tracks:\n",
        "        for msg in track:\n",
        "            timesteps += msg.time\n",
        "            if msg.type == 'note_on':\n",
        "                tmp = previous_note[msg.note]\n",
        "                tensor[tmp:timesteps, msg.note] = tensor[tmp, msg.note]\n",
        "                if binary_velocity:\n",
        "                  tensor[timesteps, msg.note] = 1 if msg.velocity > 0 else 0\n",
        "                else:\n",
        "                  tensor[timesteps, msg.note] = msg.velocity\n",
        "                previous_note[msg.note] = timesteps\n",
        "            if msg.type == 'note_off':\n",
        "                tmp = previous_note[msg.note]\n",
        "                tensor[tmp:timesteps, msg.note] = tensor[tmp, msg.note]\n",
        "                tensor[timesteps, msg.note] = 0\n",
        "                previous_note[msg.note] = timesteps\n",
        "    return torch.from_numpy(tensor)\n",
        "\n",
        "def arry2mid(ary, tempo=500000):\n",
        "    new_ary = np.concatenate([np.array([[0] * 128]), np.array(ary)], axis=0)\n",
        "    mid_new = mido.MidiFile()\n",
        "    track = mido.MidiTrack()\n",
        "    mid_new.tracks.append(track)\n",
        "    track.append(mido.MetaMessage('set_tempo', tempo=tempo, time=0))\n",
        "    changes = new_ary[1:] - new_ary[:-1]\n",
        "    last_time = 0\n",
        "    for ch in changes:\n",
        "        if sum(ch) == 0:  # no change\n",
        "            last_time += 1\n",
        "        else:\n",
        "            on_notes = np.where(ch > 0)[0]\n",
        "            on_notes_velocity = ch[on_notes]\n",
        "            off_notes = np.where(ch < 0)[0]\n",
        "            first_ = True\n",
        "            for n, v in zip(on_notes, on_notes_velocity):\n",
        "                new_time = last_time if first_ else 0\n",
        "                track.append(mido.Message('note_on', note=int(n), velocity=int(v), time=int(new_time)))\n",
        "                first_ = False\n",
        "            for n in off_notes:\n",
        "                new_time = last_time if first_ else 0\n",
        "                track.append(mido.Message('note_off', note=int(n), velocity=0, time=int(new_time)))\n",
        "                first_ = False\n",
        "            last_time = 0\n",
        "    return mid_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq84Q7NB8TcH"
      },
      "source": [
        "## Tonnetz Adjancy matrix generating function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D-nSJY3O8Nsy"
      },
      "outputs": [],
      "source": [
        "# In Tonnetz, each node has six neighbours which have pitches of the following distances (in semi-tones)\n",
        "# E.g. C4 has neighbours F3, G#3, A3, D#4, E4, G4\n",
        "NEIGHBOUR_DISTANCES = [-7, -4, -3, 3, 4, 7]\n",
        "\n",
        "def create_tonnetz_adjacency_matrix(num_notes):\n",
        "  A = []\n",
        "\n",
        "  for i in range(num_notes):\n",
        "    row = torch.zeros(num_notes, dtype=torch.int)\n",
        "    for d in NEIGHBOUR_DISTANCES:\n",
        "      j = i+d\n",
        "      if j >= 0 and j < num_notes:\n",
        "        row[j] = 1\n",
        "    A.append(row)\n",
        "  return torch.stack(A)\n",
        "\n",
        "def get_edge_attributes(A):\n",
        "  # Returns edge attributes for the adjacency matrix `A`, \n",
        "  # where the edge attributes are a one-hot encoding for each type of edge\n",
        "  edge_index = A.to_sparse().indices()\n",
        "  edge_attr_raw = []\n",
        "  for i in range(edge_index.shape[1]):\n",
        "    distance = (edge_index[1][i] - edge_index[0][i]).item()\n",
        "    edge_attr_raw.append(NEIGHBOUR_DISTANCES.index(distance))\n",
        "  return F.one_hot(torch.tensor(edge_attr_raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME2CAHxq8uLq"
      },
      "source": [
        "## Data processing helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BKqwyq5r8tZp"
      },
      "outputs": [],
      "source": [
        "import math, logging\n",
        "\n",
        "def calculate_compress_factor(file_name, desired_tpb):\n",
        "  mid = mido.MidiFile(file_name)\n",
        "  compress_factor = mid.ticks_per_beat / desired_tpb\n",
        "  if compress_factor % 1.0 != 0.0:\n",
        "    logging.warning(f\"compress_factor of {compress_factor} is not an integer, rounding up...\")\n",
        "  compress_factor = math.ceil(compress_factor)\n",
        "  return compress_factor\n",
        "\n",
        "def compress_tensor(tensor, file_name, method, desired_tpb=16):\n",
        "  '''\n",
        "  Reduces the fidelity of the musical tensor, i.e. merge multiple timesteps into one step\n",
        "\n",
        "  Args:\n",
        "    `tensor`: PyTorch tensor of shape (timesteps, num_notes)\n",
        "    `file_name`: path to the MIDI file\n",
        "    `method`: str in [\"max\", \"avg\", \"majority\"]\n",
        "    `desired_tpb`: desired ticks per beat for the tensor to be compressed to\n",
        "  '''\n",
        "  tensor_np = tensor.cpu().detach().cpu()\n",
        "  assert(len(tensor_np.shape) == 2)\n",
        "\n",
        "  compress_factor = calculate_compress_factor(file_name, desired_tpb)\n",
        "  compressed_vectors = []\n",
        "  length = tensor_np.shape[0]\n",
        "  for start in range(0, length, compress_factor):\n",
        "    end = min(start + compress_factor, length)\n",
        "    tensor_slice = tensor_np[start:end, :]\n",
        "    if (method == \"max\"):\n",
        "      raise NotImplementedError()\n",
        "    elif (method == \"avg\"):\n",
        "      raise NotImplementedError()\n",
        "    elif (method == \"majority\"):\n",
        "      majority = (end-start) / 2\n",
        "      majority_nonzeroes = np.count_nonzero(tensor_slice, axis=0) >= majority\n",
        "      compressed_vectors.append((majority_nonzeroes).astype(int))\n",
        "    else:\n",
        "      raise KeyError(f\"Unknown method {method}\")\n",
        "  return torch.tensor(np.array(compressed_vectors))\n",
        "\n",
        "def reduce_tensor(tensor, start_note, end_note):\n",
        "  '''\n",
        "  Args:\n",
        "    `tensor`: PyTorch tensor of shape (timesteps, num_notes)\n",
        "    `start_note`: note to start the tensor from (integer in 0-127)\n",
        "    `end_note`: note to end the tensor at (integer in 0-127)\n",
        "  '''\n",
        "  assert(end_note >= start_note)\n",
        "  return tensor[:, start_note:end_note+1]\n",
        "\n",
        "def uncompress_tensor(tensor, orig_tpb, compressed_tpb):\n",
        "  '''\n",
        "  Args:\n",
        "    `tensor`: PyTorch tensor of shape (timesteps, num_notes)\n",
        "    `orig_tpb`: ticks per beat of the original/generated MIDI file\n",
        "    `compressed_tpb`: ticks per beat used by the compressed `tensor`\n",
        "  '''\n",
        "  compress_factor = orig_tpb / compressed_tpb\n",
        "  if compress_factor % 1.0 != 0.0:\n",
        "    logging.warning(f\"compress_factor of {compress_factor} is not an integer, rounding up...\")\n",
        "  compress_factor = math.ceil(compress_factor)\n",
        "  # \"Stretch\" out the tensor using Kronecker product\n",
        "  return torch.kron(tensor, torch.ones((compress_factor, 1)))\n",
        "\n",
        "def unreduce_tensor(tensor, start_note, end_note):\n",
        "  '''\n",
        "  Expands out a reduced tensor to include all 128 notes in the MIDI range\n",
        "\n",
        "  Args:\n",
        "    `tensor`: PyTorch tensor of shape (timesteps, num_notes)\n",
        "    `start_note`: MIDI note that `tensor` starts from (integer in 0-127)\n",
        "    `end_note`: MIDI note that `tensor` ends at (integer in 0-127)\n",
        "  '''\n",
        "  assert(end_note >= start_note)\n",
        "  timesteps = tensor.shape[0]\n",
        "  low_notes = torch.zeros((timesteps, start_note))\n",
        "  high_notes = torch.zeros((timesteps, 127-end_note))\n",
        "  return torch.cat((low_notes, tensor, high_notes), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m6swA5zv86v7"
      },
      "outputs": [],
      "source": [
        "# function which takes music sequence and window size(history length) and return training data.\n",
        "# music = Time_step x Num_nodes\n",
        "def slice_temporal_data(music_seq,window_size=5):\n",
        "  return [[torch.transpose(music_seq[i:i+window_size],0,1),music_seq[i+window_size].reshape(-1,1)] for i in range(len(music_seq)-window_size)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCQyHMh8_cf"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2C_jOnS9Ihv",
        "outputId": "af52301f-1975-4d1f-f979-71a45ec98d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['data/2013\\\\ORIG-MIDI_01_7_10_13_Group_MID--AUDIO_02_R3_2013_wav--1.midi', 'data/2013\\\\ORIG-MIDI_01_7_10_13_Group_MID--AUDIO_07_R3_2013_wav--1.midi', 'data/2013\\\\ORIG-MIDI_01_7_10_13_Group_MID--AUDIO_07_R3_2013_wav--2.midi', 'data/2013\\\\ORIG-MIDI_01_7_10_13_Group_MID--AUDIO_07_R3_2013_wav--3.midi', 'data/2013\\\\ORIG-MIDI_01_7_10_13_Group_MID--AUDIO_08_R3_2013_wav--1.midi', 'data/2013\\\\ORIG-MIDI_01_7_10_13_Group_MID--AUDIO_08_R3_2013_wav--2.midi', 'data/2013\\\\ORIG-MIDI_01_7_10_13_Group_MID--AUDIO_08_R3_2013_wav--3.midi', 'data/2013\\\\ORIG-MIDI_01_7_6_13_Group__MID--AUDIO_01_R1_2013_wav--1.midi', 'data/2013\\\\ORIG-MIDI_01_7_6_13_Group__MID--AUDIO_01_R1_2013_wav--2.midi', 'data/2013\\\\ORIG-MIDI_01_7_6_13_Group__MID--AUDIO_01_R1_2013_wav--3.midi']\n"
          ]
        }
      ],
      "source": [
        "# Download and extract piano MIDI files from Maestro\n",
        "import os\n",
        "dir_to_scan = \"data/2013\"\n",
        "directory_files = os.listdir(dir_to_scan)\n",
        "data_files = []\n",
        "for file in directory_files:\n",
        "  data_files.append(os.path.join(dir_to_scan, file))\n",
        "print(data_files[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed5VIAheAn6e",
        "outputId": "0aa2b6a0-24d3-443a-e84e-e80e4e8ae3bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "91419 torch.Size([64, 32]) torch.Size([64, 1])\n"
          ]
        }
      ],
      "source": [
        "desired_tpb = 8\n",
        "\n",
        "num_files = 10\n",
        "window_size = desired_tpb * 4\n",
        "start_note = 33\n",
        "end_note = 96\n",
        "\n",
        "train_data = []\n",
        "for i, data_file in enumerate(data_files):\n",
        "  midi_tensor = midi_to_tensor(data_file, binary_velocity=True)\n",
        "  compressed_tensor = compress_tensor(midi_tensor, data_file, \"majority\", desired_tpb)\n",
        "  reduced_tensor = reduce_tensor(compressed_tensor, start_note, end_note)\n",
        "  train_data += slice_temporal_data(reduced_tensor.to(torch.float), window_size)\n",
        "  if i == num_files:\n",
        "    break\n",
        "print(len(train_data), train_data[0][0].shape, train_data[0][1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp9Z44B_-KRE"
      },
      "source": [
        "# Prepare model trainig "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rEHG8HOCqf6"
      },
      "source": [
        "## create graph representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3JqrzX8-PzK",
        "outputId": "b9cb4470-a71b-4bd2-fde4-1d99255b928b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 64])\n",
            "torch.Size([356, 6])\n"
          ]
        }
      ],
      "source": [
        "new_num_notes = train_data[0][0].shape[0]\n",
        "A = create_tonnetz_adjacency_matrix(new_num_notes)\n",
        "edge_attr = get_edge_attributes(A)\n",
        "print(A.shape)\n",
        "print(edge_attr.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812y9KPmCxIJ"
      },
      "source": [
        "## create data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTFnhU2eBbCF",
        "outputId": "70687f07-032f-47a8-ca8e-b3d8ad4a1b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([91419, 64, 32])\n",
            "torch.Size([91419, 64])\n",
            "torch.Size([73135, 64, 32])\n",
            "torch.Size([73135, 64])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_X = torch.stack([d[0] for d in train_data])\n",
        "print(data_X.shape)\n",
        "y = torch.stack([d[1] for d in train_data])\n",
        "data_Y = y.squeeze(-1)\n",
        "print(data_Y.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_X.to(torch.float), data_Y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "valid_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 1\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylfWFDLaCzyL"
      },
      "source": [
        "## train and eval helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s5vNRMVMB32i"
      },
      "outputs": [],
      "source": [
        "def train_eval(net, device, criterion, optimizer, epoch, train_dataloader, valid_dataloader=None, epochs_to_report=10, save_path=None):\n",
        "  '''\n",
        "  `epochs_to_report`: the number of epochs before reporting progress\n",
        "  '''\n",
        "  train_loss_list = []\n",
        "  train_acc_list = []\n",
        "  test_loss_list = []\n",
        "  test_acc_list = []\n",
        "\n",
        "  if save_path is not None:\n",
        "    torch.save(net.state_dict(), save_path)\n",
        "\n",
        "  for i in range(epoch):\n",
        "      report_progress = (i+1) % epochs_to_report == 0\n",
        "      if report_progress:\n",
        "        print(\"\\n------------------------------------------------------\")\n",
        "        print(\"Epoch {}/{}\".format(i+1,epoch))\n",
        "      \n",
        "      train_loss = 0\n",
        "      train_acc = 0\n",
        "      test_loss = 0\n",
        "      test_acc = 0\n",
        "      \n",
        "      net.train()\n",
        "      for data,label in train_dataloader:\n",
        "          data = data.to(device)\n",
        "          label = label.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          y_pred_prob = net(data[0])\n",
        "          #y_pred_prob = net(data)\n",
        "          y_pred_prob = y_pred_prob.reshape(1,-1)\n",
        "          #print(y_pred_prob.shape)\n",
        "          #print(label.shape)\n",
        "          loss = criterion(y_pred_prob,label)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss += loss.item()\n",
        "          y_pred = torch.round(torch.sigmoid(y_pred_prob))\n",
        "          train_acc += torch.count_nonzero(torch.all(torch.eq(label, y_pred), dim=1)).item() / len(label)\n",
        "      batch_train_loss = train_loss/len(train_dataloader)\n",
        "      batch_train_acc = train_acc/len(train_dataloader)\n",
        "      train_loss_list.append(batch_train_loss)\n",
        "      train_acc_list.append(batch_train_acc)\n",
        "      if report_progress:\n",
        "        print(\"Train_Loss: {:.4f} Train_Acc: {:.4f}\".format(batch_train_loss, batch_train_acc))\n",
        "      \n",
        "      if valid_dataloader is not None:\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            for data,label in valid_dataloader:\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "                y_pred_prob = net(data[0])\n",
        "                y_pred_prob = y_pred_prob.reshape(1,-1)\n",
        "                loss = criterion(y_pred_prob,label)\n",
        "                test_loss += loss.item()\n",
        "                y_pred = torch.round(torch.sigmoid(y_pred_prob))\n",
        "                test_acc += torch.count_nonzero(torch.all(torch.eq(label, y_pred), dim=1)).item() / len(label)\n",
        "        batch_test_loss = test_loss/len(valid_dataloader)\n",
        "        batch_test_acc = test_acc/len(valid_dataloader)\n",
        "        test_loss_list.append(batch_test_loss)\n",
        "        test_acc_list.append(batch_test_acc)\n",
        "        if save_path is not None:\n",
        "           torch.save(net.state_dict(), save_path)\n",
        "        if report_progress:\n",
        "          print(\"Test_Loss: {:.4f} Test_Acc: {:.4f}\".format(batch_test_loss,batch_test_acc))\n",
        "  return train_loss_list, test_loss_list, train_acc_list, test_acc_list"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hDx5UXs-C7nw"
      },
      "source": [
        "# Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNBPgThXC9mt",
        "outputId": "d0f6b951-4254-4208-a96a-1fd6f5657ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device is  cuda\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric_temporal.nn.recurrent import GConvLSTM\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device is \",device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsaFhODGGJpX"
      },
      "source": [
        "## model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ote6_xgTGM-S"
      },
      "outputs": [],
      "source": [
        "class SimpleSTGCN(nn.Module):\n",
        "    # input_dim = dim of node features\n",
        "    # output_dim = number of class which is ON/OFF\n",
        "    def __init__(self, adj_mat, edge_attrs, input_dim, hidden_dim=16, output_dim=2):\n",
        "        super(SimpleSTGCN, self).__init__()\n",
        "        self.adj_mat = adj_mat\n",
        "        self.edge_index = self.adj_mat.to_sparse().indices().to(device)\n",
        "        self.edge_weight = edge_attrs.to(device).to(float)\n",
        "        \n",
        "        # GCN layers\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.gcn_conv1 = GConvLSTM(hidden_dim,hidden_dim,K=2)   # K=2 for 2-degree convolution (neighbor convolution)\n",
        "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear4 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        #print(x.shape)\n",
        "        h,c = self.gcn_conv1(x,self.edge_index)\n",
        "        #print(h.shape, c.shape)\n",
        "        x = F.relu(self.linear3(h))\n",
        "        x = self.linear4(x)\n",
        "        y_hat = x.squeeze(-1)\n",
        "        return y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpQ8slkGL05z",
        "outputId": "70e373a1-a18b-45a5-c9cb-7d019e45965c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 32])\n",
            "torch.Size([64, 16]) torch.Size([64, 16])\n"
          ]
        }
      ],
      "source": [
        "gcn_conv1 = GConvLSTM(32,16,K=2)\n",
        "edge_index = A.to_sparse().indices()\n",
        "#print(edge_index)\n",
        "for x,y in train_dataloader:\n",
        "  print(x[0].shape)\n",
        "  h,c = gcn_conv1(x[0],edge_index)\n",
        "  print(h.shape,c.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvTIedJKIueP"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXAeKJt3IyaB",
        "outputId": "5f5d8844-4a99-4cef-8d26-79772604681a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------------------------\n",
            "Epoch 1/10\n"
          ]
        }
      ],
      "source": [
        "net = SimpleSTGCN(A, edge_attr, window_size, output_dim=1).to(device)\n",
        "pos_weight = torch.ones([new_num_notes]) * 5\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.005)\n",
        "epoch = 10\n",
        "save_path = \"model_params/Simple_GConvLSTM_model.net\"\n",
        "torch.save(net.state_dict(), save_path)\n",
        "\n",
        "trainL,testL,trainA,testA = train_eval(net, device, criterion, optimizer, epoch, train_dataloader, valid_dataloader, epochs_to_report=1, save_path=save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot(epoch,train_loss_list, test_loss_list, train_acc_list, test_acc_list):\n",
        "  # visualizing training process\n",
        "  plt.figure()\n",
        "  plt.title(\"Train and Test Loss\")\n",
        "  plt.xlabel(\"epoch\")\n",
        "  plt.ylabel(\"loss\")\n",
        "  plt.plot(range(1,epoch+1),train_loss_list,color=\"b\",linestyle=\"-\",label=\"train_loss\")\n",
        "  plt.plot(range(1,epoch+1),test_loss_list,color=\"r\",linestyle=\"--\",label=\"test_loss\")\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.title(\"Train and Test Accuracy\")\n",
        "  plt.xlabel(\"epoch\")\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.plot(range(1,epoch+1),train_acc_list,color=\"b\",linestyle=\"-\",label=\"train_accuracy\")\n",
        "  plt.plot(range(1,epoch+1),test_acc_list,color=\"r\",linestyle=\"--\",label=\"test_accuracy\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(epoch,trainL,testL,trainA,testA)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "g0-D7kWW7wIw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
